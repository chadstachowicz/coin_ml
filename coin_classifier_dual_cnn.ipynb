{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coin Grade Classifier - Dual-Image Custom CNN\n",
        "\n",
        "This notebook trains a custom CNN model from scratch to classify coin grades using **both obverse and reverse** images at full **1000x1000 resolution**.\n",
        "\n",
        "## Key Features:\n",
        "- ðŸª™ **Dual-Image Input**: Uses both sides of each coin\n",
        "- ðŸ” **Full Resolution**: 1000x1000 pixels (preserves fine details)\n",
        "- ðŸ—ï¸ **Custom Architecture**: VGG-style CNN built from scratch\n",
        "- ðŸ”„ **Feature Fusion**: Combines information from both sides\n",
        "- ðŸ’ª **No Pretrained Weights**: Trained from scratch on coin data\n",
        "\n",
        "## Architecture:\n",
        "```\n",
        "Obverse (1000x1000)     Reverse (1000x1000)\n",
        "        â†“                       â†“\n",
        "   CNN Encoder             CNN Encoder\n",
        "    (VGG-style)             (VGG-style)\n",
        "        â†“                       â†“\n",
        "        â””â”€â”€â”€â”€ Concatenate â”€â”€â”€â”€â”€â”€â”˜\n",
        "                 â†“\n",
        "           Fusion Layer\n",
        "             (1024)\n",
        "                 â†“\n",
        "       Classification Head\n",
        "                 â†“\n",
        "            Coin Grade\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
            "    app.launch_new_instance()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
            "    app.start()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
            "    handle._run()\n",
            "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
            "    await result\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
            "    result = runner(coro)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "    if await self.run_code(code, result, async_=asy):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/var/folders/dz/nxctz6qd4vj7j6jl2tv7jwn00000gn/T/ipykernel_89719/3901338260.py\", line 15, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py\", line 159, in <module>\n",
            "    from . import _api, _version, cbook, _docstring, rcsetup\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/rcsetup.py\", line 28, in <module>\n",
            "    from matplotlib.colors import Colormap, is_color_like\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/colors.py\", line 57, in <module>\n",
            "    from matplotlib import _api, _cm, cbook, scale\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/scale.py\", line 22, in <module>\n",
            "    from matplotlib.ticker import (\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/ticker.py\", line 144, in <module>\n",
            "    from matplotlib import transforms as mtransforms\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/matplotlib/transforms.py\", line 49, in <module>\n",
            "    from matplotlib._path import (\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "initialization failed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA available: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/__init__.py:159\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/rcsetup.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/colors.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/scale.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/ticker.py:144\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    146\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    148\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    149\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    150\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/matplotlib/transforms.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: initialization failed"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import json\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    print(\"MPS (Apple Silicon) available: True\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Adjust these settings based on your hardware and dataset:\n",
        "- **IMAGE_SIZE**: Keep at 1000 for full resolution\n",
        "- **BATCH_SIZE**: Increase if you have more GPU memory (16GB+ â†’ use 4)\n",
        "- **NUM_EPOCHS**: More epochs = better training (if not overfitting)\n",
        "- **DATA_DIR**: Choose 'Proof' or 'Circulation' subfolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths\n",
        "DATA_DIR = 'davidlawrence_dataset/Proof'  # or 'davidlawrence_dataset/Circulation'\n",
        "OUTPUT_DIR = 'models'\n",
        "LOG_DIR = 'runs/dual_cnn_' + datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Model hyperparameters\n",
        "IMAGE_SIZE = 512  # Full resolution\n",
        "BATCH_SIZE = 32     # Small batch due to large images\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# Device selection\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device('cuda')\n",
        "    print(\"Using CUDA (NVIDIA GPU)\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = torch.device('mps')\n",
        "    print(\"Using MPS (Apple Silicon GPU)\")\n",
        "else:\n",
        "    DEVICE = torch.device('cpu')\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "NUM_WORKERS = 0\n",
        "PIN_MEMORY = True if torch.cuda.is_available() else False\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"\\nDevice: {DEVICE}\")\n",
        "print(f\"Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"Data directory: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Class - Dual Image Loader\n",
        "\n",
        "Loads both obverse and reverse images for each coin.\n",
        "\n",
        "**Important**: Obverse and reverse images must have matching filenames!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DualCoinDataset(Dataset):\n",
        "    \"\"\"Dataset that loads both obverse and reverse images for each coin.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, split='train', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Root directory with structure: <grade>/obverse/ and <grade>/reverse/\n",
        "            split: 'train', 'test', or 'val'\n",
        "            transform: Optional transform to apply to both images\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "        self.idx_to_class = {}\n",
        "        \n",
        "        # Scan for grade folders\n",
        "        grade_folders = sorted([d for d in self.data_dir.iterdir() if d.is_dir()])\n",
        "        \n",
        "        for idx, grade_folder in enumerate(grade_folders):\n",
        "            grade_name = grade_folder.name\n",
        "            self.class_to_idx[grade_name] = idx\n",
        "            self.idx_to_class[idx] = grade_name\n",
        "            \n",
        "            obverse_dir = grade_folder / 'obverse'\n",
        "            reverse_dir = grade_folder / 'reverse'\n",
        "            \n",
        "            if not obverse_dir.exists() or not reverse_dir.exists():\n",
        "                print(f\"Warning: Missing obverse or reverse folder for {grade_name}\")\n",
        "                continue\n",
        "            \n",
        "            # Get all obverse images\n",
        "            obverse_images = sorted([f for f in obverse_dir.glob('*.jpg') if f.is_file()])\n",
        "            \n",
        "            for obverse_img in obverse_images:\n",
        "                # Find matching reverse image (same filename)\n",
        "                reverse_img = reverse_dir / obverse_img.name\n",
        "                \n",
        "                if reverse_img.exists():\n",
        "                    self.samples.append({\n",
        "                        'obverse': obverse_img,\n",
        "                        'reverse': reverse_img,\n",
        "                        'label': idx,\n",
        "                        'grade': grade_name\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"Warning: No matching reverse for {obverse_img.name}\")\n",
        "        \n",
        "        # Split data (70% train, 20% test, 10% val)\n",
        "        np.random.seed(42)\n",
        "        indices = np.random.permutation(len(self.samples))\n",
        "        \n",
        "        n_train = int(0.7 * len(self.samples))\n",
        "        n_test = int(0.2 * len(self.samples))\n",
        "        \n",
        "        if split == 'train':\n",
        "            indices = indices[:n_train]\n",
        "        elif split == 'test':\n",
        "            indices = indices[n_train:n_train + n_test]\n",
        "        else:  # val\n",
        "            indices = indices[n_train + n_test:]\n",
        "        \n",
        "        self.samples = [self.samples[i] for i in indices]\n",
        "        \n",
        "        print(f\"{split.upper()} Dataset:\")\n",
        "        print(f\"  Total samples: {len(self.samples)}\")\n",
        "        print(f\"  Classes: {len(self.class_to_idx)}\")\n",
        "        print(f\"  Class names: {list(self.class_to_idx.keys())}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        \n",
        "        # Load both images\n",
        "        obverse = Image.open(sample['obverse']).convert('RGB')\n",
        "        reverse = Image.open(sample['reverse']).convert('RGB')\n",
        "        \n",
        "        # Apply same transforms to both\n",
        "        if self.transform:\n",
        "            obverse = self.transform(obverse)\n",
        "            reverse = self.transform(reverse)\n",
        "        \n",
        "        return obverse, reverse, sample['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Transforms\n",
        "\n",
        "Minimal transforms to preserve coin details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training transforms - minimal augmentation to preserve detail\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation/test transforms - no augmentation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"âœ“ Transforms configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create Datasets and DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_dataset = DualCoinDataset(DATA_DIR, split='train', transform=train_transform)\n",
        "test_dataset = DualCoinDataset(DATA_DIR, split='test', transform=val_transform)\n",
        "val_dataset = DualCoinDataset(DATA_DIR, split='val', transform=val_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY\n",
        ")\n",
        "\n",
        "print(f\"\\nDataloaders created:\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Custom Dual CNN Model\n",
        "\n",
        "VGG-style architecture with feature fusion for dual images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DualCNNClassifier(nn.Module):\n",
        "    \"\"\"Custom CNN for dual-image (obverse + reverse) classification.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_classes):\n",
        "        super(DualCNNClassifier, self).__init__()\n",
        "        \n",
        "        # CNN encoder for processing each image\n",
        "        # VGG-style architecture with batch norm\n",
        "        def make_encoder():\n",
        "            return nn.Sequential(\n",
        "                # Block 1: 1000x1000 -> 500x500\n",
        "                nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                \n",
        "                # Block 2: 500x500 -> 250x250\n",
        "                nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(128),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                \n",
        "                # Block 3: 250x250 -> 125x125\n",
        "                nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(256),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                \n",
        "                # Block 4: 125x125 -> 62x62\n",
        "                nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                \n",
        "                # Block 5: 62x62 -> 31x31\n",
        "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "                nn.BatchNorm2d(512),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "                \n",
        "                # Global average pooling\n",
        "                nn.AdaptiveAvgPool2d((1, 1))\n",
        "            )\n",
        "        \n",
        "        # Separate encoders for obverse and reverse\n",
        "        self.obverse_encoder = make_encoder()\n",
        "        self.reverse_encoder = make_encoder()\n",
        "        \n",
        "        # Fusion layer - combines features from both sides\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(512 * 2, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "    \n",
        "    def forward(self, obverse, reverse):\n",
        "        # Process obverse image\n",
        "        obverse_feat = self.obverse_encoder(obverse)  # [B, 512, 1, 1]\n",
        "        obverse_feat = obverse_feat.view(obverse_feat.size(0), -1)  # [B, 512]\n",
        "        \n",
        "        # Process reverse image\n",
        "        reverse_feat = self.reverse_encoder(reverse)  # [B, 512, 1, 1]\n",
        "        reverse_feat = reverse_feat.view(reverse_feat.size(0), -1)  # [B, 512]\n",
        "        \n",
        "        # Concatenate features from both sides\n",
        "        combined = torch.cat([obverse_feat, reverse_feat], dim=1)  # [B, 1024]\n",
        "        \n",
        "        # Fusion\n",
        "        fused = self.fusion(combined)  # [B, 1024]\n",
        "        \n",
        "        # Classification\n",
        "        output = self.classifier(fused)  # [B, num_classes]\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Create model\n",
        "num_classes = len(train_dataset.class_to_idx)\n",
        "model = DualCNNClassifier(num_classes=num_classes)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "print(f\"\\nModel created:\")\n",
        "print(f\"  Number of classes: {num_classes}\")\n",
        "print(f\"  Classes: {list(train_dataset.class_to_idx.keys())}\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "print(f\"  Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    mode='max', \n",
        "    factor=0.5, \n",
        "    patience=5, \n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# TensorBoard writer\n",
        "writer = SummaryWriter(LOG_DIR)\n",
        "\n",
        "print(\"Training setup complete\")\n",
        "print(f\"  Optimizer: Adam\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
        "print(f\"  TensorBoard logs: {LOG_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device, epoch):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [Train]')\n",
        "    \n",
        "    for obverse, reverse, labels in pbar:\n",
        "        obverse = obverse.to(device)\n",
        "        reverse = reverse.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(obverse, reverse)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item() * obverse.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100 * correct / total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device, split='Val'):\n",
        "    \"\"\"Validate the model.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(loader, desc=f'{split}')\n",
        "        for obverse, reverse, labels in pbar:\n",
        "            obverse = obverse.to(device)\n",
        "            reverse = reverse.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(obverse, reverse)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            running_loss += loss.item() * obverse.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "print(\"âœ“ Training functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Main Training Loop\n",
        "\n",
        "**Run this cell to start training!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_path = os.path.join(OUTPUT_DIR, 'coin_dual_cnn_best.pth')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Train\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, DEVICE, epoch)\n",
        "    \n",
        "    # Validate\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, DEVICE, split='Val')\n",
        "    \n",
        "    # Update learning rate\n",
        "    scheduler.step(val_acc)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    \n",
        "    # Log to TensorBoard\n",
        "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "    writer.add_scalar('Accuracy/val', val_acc, epoch)\n",
        "    writer.add_scalar('Learning_rate', current_lr, epoch)\n",
        "    \n",
        "    # Print summary\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "    print(f\"  LR: {current_lr:.6f}\")\n",
        "    \n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'class_to_idx': train_dataset.class_to_idx,\n",
        "            'idx_to_class': train_dataset.idx_to_class\n",
        "        }, best_model_path)\n",
        "        print(f\"  âœ“ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"Best model saved to: {best_model_path}\")\n",
        "\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Plot Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss plot\n",
        "ax1.plot(history['train_loss'], label='Train Loss', marker='o')\n",
        "ax1.plot(history['val_loss'], label='Val Loss', marker='s')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Accuracy plot\n",
        "ax2.plot(history['train_acc'], label='Train Acc', marker='o')\n",
        "ax2.plot(history['val_acc'], label='Val Acc', marker='s')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history_dual_cnn.png'), dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"Training plots saved to: {OUTPUT_DIR}/training_history_dual_cnn.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_loss, test_acc = validate(model, test_loader, criterion, DEVICE, split='Test')\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL TEST RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. View TensorBoard Logs\n",
        "\n",
        "Run this command in terminal to view training progress:\n",
        "\n",
        "```bash\n",
        "tensorboard --logdir=runs\n",
        "```\n",
        "\n",
        "Then open http://localhost:6006"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Save Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save training history\n",
        "with open(os.path.join(OUTPUT_DIR, 'history_dual_cnn.json'), 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "# Save configuration\n",
        "config = {\n",
        "    'image_size': IMAGE_SIZE,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'num_epochs': NUM_EPOCHS,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'num_classes': num_classes,\n",
        "    'classes': list(train_dataset.class_to_idx.keys()),\n",
        "    'best_val_acc': best_val_acc,\n",
        "    'test_acc': test_acc\n",
        "}\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'config_dual_cnn.json'), 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "print(\"\\nResults saved:\")\n",
        "print(f\"  Model: {best_model_path}\")\n",
        "print(f\"  History: {OUTPUT_DIR}/history_dual_cnn.json\")\n",
        "print(f\"  Config: {OUTPUT_DIR}/config_dual_cnn.json\")\n",
        "print(f\"  Plots: {OUTPUT_DIR}/training_history_dual_cnn.png\")\n",
        "print(f\"\\nâœ“ All done! Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "print(f\"âœ“ Test accuracy: {test_acc:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
